{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework 2: Topic Modeling of COVID-19 Tweets Over Time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this homework, you will be using topic modeling to identify topics in tweets about COVID-19 collected from February through April. You will use PCA to visualize the results of your topic modeling as well as investigate the semantic meaning of the topics you have found and the prevalence of these topics over time. To complete this homework assignment, run each block of code, filling in code as needed. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading Tweet Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tweets = pd.read_csv('april_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>date</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>100</th>\n",
       "      <td>100</td>\n",
       "      <td>2020-04-05 23:38:39</td>\n",
       "      <td>#7: Leading public health official who dealt w...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>101</th>\n",
       "      <td>101</td>\n",
       "      <td>2020-04-05 23:37:42</td>\n",
       "      <td>tyler joseph explaining to his daughter how he...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>102</th>\n",
       "      <td>102</td>\n",
       "      <td>2020-04-05 23:37:37</td>\n",
       "      <td>Trump is attacking the Governor of Illinois, m...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>103</th>\n",
       "      <td>103</td>\n",
       "      <td>2020-04-05 23:37:08</td>\n",
       "      <td>Everyone with the Coronavirus</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>104</th>\n",
       "      <td>104</td>\n",
       "      <td>2020-04-05 23:37:05</td>\n",
       "      <td>Fauci says it's \"false\" to say US has coronavi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>105</th>\n",
       "      <td>105</td>\n",
       "      <td>2020-04-05 23:37:03</td>\n",
       "      <td>Just asking. Is anyone sanitizing those ventil...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>106</th>\n",
       "      <td>106</td>\n",
       "      <td>2020-04-05 23:36:48</td>\n",
       "      <td>My county did a thing where everyone was suppo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>107</th>\n",
       "      <td>107</td>\n",
       "      <td>2020-04-05 23:36:33</td>\n",
       "      <td>Native American communities in rural areas are...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>108</th>\n",
       "      <td>108</td>\n",
       "      <td>2020-04-05 23:35:54</td>\n",
       "      <td>Before and After of our quarantine makeover</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>109</th>\n",
       "      <td>109</td>\n",
       "      <td>2020-04-05 23:35:32</td>\n",
       "      <td>then I told mr. corona to post up</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>110</th>\n",
       "      <td>110</td>\n",
       "      <td>2020-04-05 23:35:22</td>\n",
       "      <td>Blackberry peach cobbler.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>111</th>\n",
       "      <td>111</td>\n",
       "      <td>2020-04-05 23:35:10</td>\n",
       "      <td>Trump is a draft dodging, woman raping, daught...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>112</th>\n",
       "      <td>112</td>\n",
       "      <td>2020-04-05 23:35:04</td>\n",
       "      <td>Coronavirus Forces Landlord To Cut Back On Tak...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>113</th>\n",
       "      <td>113</td>\n",
       "      <td>2020-04-05 23:35:00</td>\n",
       "      <td>The 'solidarity' basket ascends and descends ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>114</th>\n",
       "      <td>114</td>\n",
       "      <td>2020-04-05 23:34:52</td>\n",
       "      <td>The President’s need to be on camera, forces h...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>115</th>\n",
       "      <td>115</td>\n",
       "      <td>2020-04-05 23:34:26</td>\n",
       "      <td>Rear Adm. John Polowczy said today at briefing...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>116</th>\n",
       "      <td>116</td>\n",
       "      <td>2020-04-05 23:33:38</td>\n",
       "      <td>no one is safe #covid19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>117</th>\n",
       "      <td>117</td>\n",
       "      <td>2020-04-05 23:32:27</td>\n",
       "      <td>Carole Fuckin Baskin. https://nypost.com/2020/...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>118</th>\n",
       "      <td>118</td>\n",
       "      <td>2020-04-05 23:33:31</td>\n",
       "      <td>Today’s #StayHomeForNevada lesson with the kid...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>119</th>\n",
       "      <td>119</td>\n",
       "      <td>2020-04-05 23:30:17</td>\n",
       "      <td>Here @whpresscorps Ask Trump about THIS!! \"Giu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>120</th>\n",
       "      <td>120</td>\n",
       "      <td>2020-04-05 23:29:00</td>\n",
       "      <td>During our Family Town Hall on COVID-19, I was...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>121</th>\n",
       "      <td>121</td>\n",
       "      <td>2020-04-05 23:28:52</td>\n",
       "      <td>Deaths from #coronavirus are not our generatio...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>122</th>\n",
       "      <td>122</td>\n",
       "      <td>2020-04-05 23:28:28</td>\n",
       "      <td>President @realDonaldTrump asks for all Americ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>123</th>\n",
       "      <td>123</td>\n",
       "      <td>2020-04-05 23:28:22</td>\n",
       "      <td>Did the Deep State intentionally cause the Cov...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>124</th>\n",
       "      <td>124</td>\n",
       "      <td>2020-04-05 23:26:54</td>\n",
       "      <td>VGMA: Char this year no SM no Bhim CORONA: pic...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>125</th>\n",
       "      <td>125</td>\n",
       "      <td>2020-04-05 23:26:52</td>\n",
       "      <td>BALBRIGGAN WOMEN'S DEVELOPMENT GROUP IS CAMPAI...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>126</th>\n",
       "      <td>126</td>\n",
       "      <td>2020-04-05 23:26:26</td>\n",
       "      <td>Our Challenge: “Massive Enforcement from MOSQU...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>127</th>\n",
       "      <td>127</td>\n",
       "      <td>2020-04-05 23:26:04</td>\n",
       "      <td>Ok, China was suddenly thrust into a lethal pa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>128</th>\n",
       "      <td>128</td>\n",
       "      <td>2020-04-05 23:25:38</td>\n",
       "      <td>Thank you to CBS for choosing to broadcast 60 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>129</th>\n",
       "      <td>129</td>\n",
       "      <td>2020-04-05 23:25:11</td>\n",
       "      <td>The Career Highlights of Dr. Fauci in 2020: Ja...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>170</th>\n",
       "      <td>170</td>\n",
       "      <td>2020-04-05 23:08:22</td>\n",
       "      <td>#coronavirus #mask creativity</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>171</th>\n",
       "      <td>171</td>\n",
       "      <td>2020-04-05 23:08:19</td>\n",
       "      <td>Nearly 10,000 Americans have died of Covid-19 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>172</th>\n",
       "      <td>172</td>\n",
       "      <td>2020-04-05 23:07:13</td>\n",
       "      <td>#Thread: Experts challenging government offici...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>173</th>\n",
       "      <td>173</td>\n",
       "      <td>2020-04-05 23:07:04</td>\n",
       "      <td>Why do I hear Cardi B say “Coronavirus” every ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>174</th>\n",
       "      <td>174</td>\n",
       "      <td>2020-04-05 23:05:59</td>\n",
       "      <td>Democrat Dementia Sufferer #QuidPro Sleepy @Jo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>175</th>\n",
       "      <td>175</td>\n",
       "      <td>2020-04-05 23:05:30</td>\n",
       "      <td>Caraga is no longer COVID-FREE.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>176</th>\n",
       "      <td>176</td>\n",
       "      <td>2020-04-05 23:04:59</td>\n",
       "      <td>I am 99% sure this is how COVID got started...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>177</th>\n",
       "      <td>177</td>\n",
       "      <td>2020-04-05 23:04:46</td>\n",
       "      <td>When this is all over there will be members of...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>178</th>\n",
       "      <td>178</td>\n",
       "      <td>2020-04-05 23:04:19</td>\n",
       "      <td>Updated MOH Dashboard. 4 Recovered Cases. #Cov...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>179</th>\n",
       "      <td>179</td>\n",
       "      <td>2020-04-05 23:04:13</td>\n",
       "      <td>We must STOP THIS GLOOM AND DOOM promoting Fea...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>180</th>\n",
       "      <td>180</td>\n",
       "      <td>2020-04-05 23:04:01</td>\n",
       "      <td>Coronavirus: Pastor who decried 'hysteria' die...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>181</th>\n",
       "      <td>181</td>\n",
       "      <td>2020-04-05 23:02:54</td>\n",
       "      <td>F*ck you Corona</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>182</th>\n",
       "      <td>182</td>\n",
       "      <td>2020-04-05 23:02:08</td>\n",
       "      <td>Mahama Right Now. #nanaaddo #corona #ghana</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>183</th>\n",
       "      <td>183</td>\n",
       "      <td>2020-04-05 23:00:49</td>\n",
       "      <td>Introducing the Coronaboo, with 2 Corona Beers...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>184</th>\n",
       "      <td>184</td>\n",
       "      <td>2020-04-05 23:00:41</td>\n",
       "      <td>SNP Socialists statement on Covid-19 As social...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>185</th>\n",
       "      <td>185</td>\n",
       "      <td>2020-04-05 23:00:33</td>\n",
       "      <td>.* Feb 29: first US death from the coronavirus...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>186</th>\n",
       "      <td>186</td>\n",
       "      <td>2020-04-05 23:00:00</td>\n",
       "      <td>More than 12% of Pennsylvania’s labor force fi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>187</th>\n",
       "      <td>187</td>\n",
       "      <td>2020-04-05 23:00:00</td>\n",
       "      <td>NYC ER Dr. Cameron Kyle-Sidell isn’t suggestin...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>188</th>\n",
       "      <td>188</td>\n",
       "      <td>2020-04-05 22:59:44</td>\n",
       "      <td>There is no way in HELL that China had only 33...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>189</th>\n",
       "      <td>189</td>\n",
       "      <td>2020-04-05 22:59:30</td>\n",
       "      <td>Wow. Irish Prime Minister Leo Varadkar has re-...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>190</th>\n",
       "      <td>190</td>\n",
       "      <td>2020-04-05 22:59:00</td>\n",
       "      <td>During these uncertain times, getting the righ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>191</th>\n",
       "      <td>191</td>\n",
       "      <td>2020-04-05 22:58:27</td>\n",
       "      <td>IRELAND: Irish PM re-registers as doctor to he...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>192</th>\n",
       "      <td>192</td>\n",
       "      <td>2020-04-05 22:57:28</td>\n",
       "      <td>The world is a different place than it was a f...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>193</th>\n",
       "      <td>193</td>\n",
       "      <td>2020-04-05 22:57:22</td>\n",
       "      <td>If Coronavirus was a person:D</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>194</th>\n",
       "      <td>194</td>\n",
       "      <td>2020-04-05 22:55:57</td>\n",
       "      <td>Bill Gates: Funded The Flawed #CoronaVirus Mod...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>195</th>\n",
       "      <td>195</td>\n",
       "      <td>2020-04-05 22:54:56</td>\n",
       "      <td>COVID-19: How to win! pdf https://www.endcoron...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>196</th>\n",
       "      <td>196</td>\n",
       "      <td>2020-04-05 22:54:55</td>\n",
       "      <td>Coronavirus: Leo Varadkar to work as doctor du...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>197</th>\n",
       "      <td>197</td>\n",
       "      <td>2020-04-05 22:59:24</td>\n",
       "      <td>Inspired by Harry &amp; Meghan, we invite you to h...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>198</th>\n",
       "      <td>198</td>\n",
       "      <td>2020-04-05 22:57:12</td>\n",
       "      <td>Sen. Tina Smith: #Trump is treating the #coron...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199</th>\n",
       "      <td>199</td>\n",
       "      <td>2020-04-05 22:54:28</td>\n",
       "      <td>Oyo Gov, Makinde Tests Negative After Testing ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Unnamed: 0                 date  \\\n",
       "100         100  2020-04-05 23:38:39   \n",
       "101         101  2020-04-05 23:37:42   \n",
       "102         102  2020-04-05 23:37:37   \n",
       "103         103  2020-04-05 23:37:08   \n",
       "104         104  2020-04-05 23:37:05   \n",
       "105         105  2020-04-05 23:37:03   \n",
       "106         106  2020-04-05 23:36:48   \n",
       "107         107  2020-04-05 23:36:33   \n",
       "108         108  2020-04-05 23:35:54   \n",
       "109         109  2020-04-05 23:35:32   \n",
       "110         110  2020-04-05 23:35:22   \n",
       "111         111  2020-04-05 23:35:10   \n",
       "112         112  2020-04-05 23:35:04   \n",
       "113         113  2020-04-05 23:35:00   \n",
       "114         114  2020-04-05 23:34:52   \n",
       "115         115  2020-04-05 23:34:26   \n",
       "116         116  2020-04-05 23:33:38   \n",
       "117         117  2020-04-05 23:32:27   \n",
       "118         118  2020-04-05 23:33:31   \n",
       "119         119  2020-04-05 23:30:17   \n",
       "120         120  2020-04-05 23:29:00   \n",
       "121         121  2020-04-05 23:28:52   \n",
       "122         122  2020-04-05 23:28:28   \n",
       "123         123  2020-04-05 23:28:22   \n",
       "124         124  2020-04-05 23:26:54   \n",
       "125         125  2020-04-05 23:26:52   \n",
       "126         126  2020-04-05 23:26:26   \n",
       "127         127  2020-04-05 23:26:04   \n",
       "128         128  2020-04-05 23:25:38   \n",
       "129         129  2020-04-05 23:25:11   \n",
       "..          ...                  ...   \n",
       "170         170  2020-04-05 23:08:22   \n",
       "171         171  2020-04-05 23:08:19   \n",
       "172         172  2020-04-05 23:07:13   \n",
       "173         173  2020-04-05 23:07:04   \n",
       "174         174  2020-04-05 23:05:59   \n",
       "175         175  2020-04-05 23:05:30   \n",
       "176         176  2020-04-05 23:04:59   \n",
       "177         177  2020-04-05 23:04:46   \n",
       "178         178  2020-04-05 23:04:19   \n",
       "179         179  2020-04-05 23:04:13   \n",
       "180         180  2020-04-05 23:04:01   \n",
       "181         181  2020-04-05 23:02:54   \n",
       "182         182  2020-04-05 23:02:08   \n",
       "183         183  2020-04-05 23:00:49   \n",
       "184         184  2020-04-05 23:00:41   \n",
       "185         185  2020-04-05 23:00:33   \n",
       "186         186  2020-04-05 23:00:00   \n",
       "187         187  2020-04-05 23:00:00   \n",
       "188         188  2020-04-05 22:59:44   \n",
       "189         189  2020-04-05 22:59:30   \n",
       "190         190  2020-04-05 22:59:00   \n",
       "191         191  2020-04-05 22:58:27   \n",
       "192         192  2020-04-05 22:57:28   \n",
       "193         193  2020-04-05 22:57:22   \n",
       "194         194  2020-04-05 22:55:57   \n",
       "195         195  2020-04-05 22:54:56   \n",
       "196         196  2020-04-05 22:54:55   \n",
       "197         197  2020-04-05 22:59:24   \n",
       "198         198  2020-04-05 22:57:12   \n",
       "199         199  2020-04-05 22:54:28   \n",
       "\n",
       "                                                  text  \n",
       "100  #7: Leading public health official who dealt w...  \n",
       "101  tyler joseph explaining to his daughter how he...  \n",
       "102  Trump is attacking the Governor of Illinois, m...  \n",
       "103                     Everyone with the Coronavirus   \n",
       "104  Fauci says it's \"false\" to say US has coronavi...  \n",
       "105  Just asking. Is anyone sanitizing those ventil...  \n",
       "106  My county did a thing where everyone was suppo...  \n",
       "107  Native American communities in rural areas are...  \n",
       "108        Before and After of our quarantine makeover  \n",
       "109                 then I told mr. corona to post up   \n",
       "110                          Blackberry peach cobbler.  \n",
       "111  Trump is a draft dodging, woman raping, daught...  \n",
       "112  Coronavirus Forces Landlord To Cut Back On Tak...  \n",
       "113   The 'solidarity' basket ascends and descends ...  \n",
       "114  The President’s need to be on camera, forces h...  \n",
       "115  Rear Adm. John Polowczy said today at briefing...  \n",
       "116                            no one is safe #covid19  \n",
       "117  Carole Fuckin Baskin. https://nypost.com/2020/...  \n",
       "118  Today’s #StayHomeForNevada lesson with the kid...  \n",
       "119  Here @whpresscorps Ask Trump about THIS!! \"Giu...  \n",
       "120  During our Family Town Hall on COVID-19, I was...  \n",
       "121  Deaths from #coronavirus are not our generatio...  \n",
       "122  President @realDonaldTrump asks for all Americ...  \n",
       "123  Did the Deep State intentionally cause the Cov...  \n",
       "124  VGMA: Char this year no SM no Bhim CORONA: pic...  \n",
       "125  BALBRIGGAN WOMEN'S DEVELOPMENT GROUP IS CAMPAI...  \n",
       "126  Our Challenge: “Massive Enforcement from MOSQU...  \n",
       "127  Ok, China was suddenly thrust into a lethal pa...  \n",
       "128  Thank you to CBS for choosing to broadcast 60 ...  \n",
       "129  The Career Highlights of Dr. Fauci in 2020: Ja...  \n",
       "..                                                 ...  \n",
       "170                      #coronavirus #mask creativity  \n",
       "171  Nearly 10,000 Americans have died of Covid-19 ...  \n",
       "172  #Thread: Experts challenging government offici...  \n",
       "173  Why do I hear Cardi B say “Coronavirus” every ...  \n",
       "174  Democrat Dementia Sufferer #QuidPro Sleepy @Jo...  \n",
       "175                   Caraga is no longer COVID-FREE.   \n",
       "176    I am 99% sure this is how COVID got started...   \n",
       "177  When this is all over there will be members of...  \n",
       "178  Updated MOH Dashboard. 4 Recovered Cases. #Cov...  \n",
       "179  We must STOP THIS GLOOM AND DOOM promoting Fea...  \n",
       "180  Coronavirus: Pastor who decried 'hysteria' die...  \n",
       "181                                   F*ck you Corona   \n",
       "182         Mahama Right Now. #nanaaddo #corona #ghana  \n",
       "183  Introducing the Coronaboo, with 2 Corona Beers...  \n",
       "184  SNP Socialists statement on Covid-19 As social...  \n",
       "185  .* Feb 29: first US death from the coronavirus...  \n",
       "186  More than 12% of Pennsylvania’s labor force fi...  \n",
       "187  NYC ER Dr. Cameron Kyle-Sidell isn’t suggestin...  \n",
       "188  There is no way in HELL that China had only 33...  \n",
       "189  Wow. Irish Prime Minister Leo Varadkar has re-...  \n",
       "190  During these uncertain times, getting the righ...  \n",
       "191  IRELAND: Irish PM re-registers as doctor to he...  \n",
       "192  The world is a different place than it was a f...  \n",
       "193                     If Coronavirus was a person:D   \n",
       "194  Bill Gates: Funded The Flawed #CoronaVirus Mod...  \n",
       "195  COVID-19: How to win! pdf https://www.endcoron...  \n",
       "196  Coronavirus: Leo Varadkar to work as doctor du...  \n",
       "197  Inspired by Harry & Meghan, we invite you to h...  \n",
       "198  Sen. Tina Smith: #Trump is treating the #coron...  \n",
       "199  Oyo Gov, Makinde Tests Negative After Testing ...  \n",
       "\n",
       "[100 rows x 3 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets.head()\n",
    "tweets[100:200]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Topic Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building Our Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To do our topic modeling, we will be using Latent Dirichlet Allocation to do topic modeling, which you learned about in lecture. Don't worry, we will not be implementing Latent Dirichlet Allocation, instead we will be using the tools already available to us (sklearn) in order to do topic modeling.  For a refresher on this method, you can check out [this](https://towardsdatascience.com/light-on-math-machine-learning-intuitive-guide-to-latent-dirichlet-allocation-437c81220158) blog post. To start we will build up some understanding of how to use sklearn's LDA functionality. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "from sklearn.feature_extraction import text\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will start out by adding to our stop words. These are words that may be repeated often in the tweets, but do not add any topic information. These are words like 'the', 'a', 'that', 'almost', 'every', etc. You can check out the existing stop words using text.ENGLISH_STOP_WORDS if you want to know what's included. What you need to do is add words that are likely not in the existing stop word list that will not add any information to our topics. Because all the tweets are already about COVID-19, words used to identify the virus are likely not helpful, nor are things like 'https' and 'com' from links. Think about what words might come up a lot but do not differentiate tweets and add them to the stop list. After you run the topic modeling and see the words in the topics, continue to add unimportant words to the stop list to refine your topics (we added about 15 additional stop words but you can add as many as you think are appropriate). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TASK 1: add stop words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "my_stops = text.ENGLISH_STOP_WORDS.union(\n",
    "    \n",
    "    ['COVID-19','covid_19', 'like' ,'19', 'coronavirus', 'pandemic', 'virus', 'https', 'com', 'net', 'cases', 'breaking', 'news', 'story', 'infection', 'link', 'covid19', 'covid']\n",
    "\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we will vectorize the tweets. This makes it so each tweet is represented by an w-dimensional vector, where w is the total number number of unique words in all the tweets (not including the stop words). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer(min_df=10, stop_words=my_stops)\n",
    "counts = vectorizer.fit_transform(tweets.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will run Latent Dirichlet Allocation to find our topics. Check out the [documentation](https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.LatentDirichletAllocation.html) in order to understand all of the parameters. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/egreenberg/anaconda/lib/python3.6/site-packages/sklearn/decomposition/online_lda.py:532: DeprecationWarning: The default value for 'learning_method' will be changed from 'online' to 'batch' in the release 0.20. This warning was introduced in 0.18.\n",
      "  DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "as_topics = LatentDirichletAllocation(n_components=8, random_state=0, n_jobs=6).fit(counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def print_topics(topics,vectorizer):\n",
    "    \"\"\"\n",
    "    Prints top 12 most important words for each topic in descending importance\n",
    "    \"\"\"\n",
    "    topic_dists = (topics.components_.T / topics.components_.sum(axis=1)).T\n",
    "    for comp in range(len(topic_dists)):\n",
    "        top_i = np.argsort(topic_dists[comp])[-12:][::-1]\n",
    "        print()\n",
    "        print([key for key, value in vectorizer.vocabulary_.items() if value in top_i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "['patients', 've', 'deaths', 'care', 'doctors', '2020', '04', 'health', 'http', 'amp', 'black', 'healthcare']\n",
      "\n",
      "['doctor', 'hydroxychloroquine', 'drug', 'trump', 'says', 'watch', 'crisis', 'use', 'www', 'medical', 'help', 'ik_saviourofnation']\n",
      "\n",
      "['tiger', 'new', 'york', 'bronx', 'zoo', 'tests', 'positive', 'tested', 'old', 'year', 'day', 'confirmed']\n",
      "\n",
      "['says', 'come', 'way', 'work', 'dr', 'response', 'need', 'got', 'fauci', 'fight', 'today', 'times']\n",
      "\n",
      "['hospital', 'deaths', 'government', 'social', 'lives', 'save', 'uk', 'died', 'lockdown', 'week', 'days', 'april']\n",
      "\n",
      "['10', 'world', 'time', 'corona', 'want', 'pic', 'twitter', 'states', 'united', 'let', 'india', 'narendramodi']\n",
      "\n",
      "['trump', 'china', 'home', 'people', 'just', 'support', '000', 'thank', 'public', 'amp', 'stay', 'safe']\n",
      "\n",
      "['trump', 'world', 'going', 'don', 'away', 'stop', 'weeks', 'vaccine', 'spread', 'know', 'god', 'death']\n"
     ]
    }
   ],
   "source": [
    "print_topics(as_topics, vectorizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating our Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is an unsupervised learning technique meant to help us find structure in our data. The data will always be divided into topics, however that does not mean that the topics will have meaning. This makes the qualitative part of topic modeling very important because you need to be able to look at the topics generated above and understand whether the model seems to have captured true structure or just noise. Now we can think about ways to visualize what we have just created. \n",
    "\n",
    "One way we can gain intuition about our model is by checking how strongly it is classifying tweets into one category or another. In other words, how sure the model is of the topic of a tweet. When predicting the topic for a tweet, the trained model returns a distribution over the possible topics, which is referred to as that tweet in _topic space_. For example, if we had three topics it would return a vector of length 3 with the 3 values summing to 1. The topic represented by the highest probability is the principal topic of that tweet. The higher that maximum value is compared to the other probabilities, the stronger the association of that tweet to its principal topic. If tweets tend to have strong principal topics, then there is a lot of structure in the topic space of the tweets and the topics are more likely to be useful.\n",
    "\n",
    "By measuring this charactaristic of our model, we are making sure our model identified more general structure in the tweets and didn't train strictly on noise. While this doesn't mean that the topics have __meaning__, it is a check that there is clustering going on. To visualize whether this clustering is happening we can plot our points and color them based on their given topic. However, our data is more than 3-dimensions, making it difficult to visualize. That's where dimensionality reduction techniques come in. There are many techniques out there, but we are going to focus on PCA as it is ubiquitous, easy to understand, and very useful. \n",
    "\n",
    "If you want to explore other demonsionality reduction tools, [tSNE](https://scikit-learn.org/stable/modules/generated/sklearn.manifold.TSNE.html) is a widely used one as well. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TASK 2: Implement PCA\n",
    "\n",
    "Before we perform PCA to visualize our topic modeling, check out [this](https://datascienceplus.com/understanding-the-covariance-matrix/) blog post on covariance matrices and [this](https://towardsdatascience.com/a-one-stop-shop-for-principal-component-analysis-5582fb7e0a9c) blog post on PCA if you are unfamiliar with either topic. To really solidify your understanding of PCA, you are going to __implement it yourself__.\n",
    "\n",
    "To do this, please fill in the provided helper functions and PCA wrapper function. You __may not__ use any built in PCA functions but you can use anything else. The following example depends on your implementation of this code so make sure to implement it before running that. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_covariance(data):\n",
    "    \"\"\"\n",
    "    Given a matrix of data, returns covariance matrix\n",
    "    \n",
    "    Hint: Make sure to standardize your data and center it around zero. \n",
    "    Try to do this in a vectorized manner, our solution was less than 5 lines. \n",
    "    \"\"\"\n",
    "    # Your code here\n",
    "    #first, make \n",
    "    n = len(data)\n",
    "    means = data.sum(axis=0)/n\n",
    "    means = np.tile(means,(n, 1))\n",
    "    X = data - means\n",
    "    covariance = (1/(n-1))*np.dot(X.transpose(), X)\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    return covariance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 0 0]\n",
      " [0 2 0]\n",
      " [0 0 3]]\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'numpy.ndarray' object has no attribute 'tranpose'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-84-94ac3771429b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0md\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdiag\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0md\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mw\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinalg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meig\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0md\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranpose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0md\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mv\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'numpy.ndarray' object has no attribute 'tranpose'"
     ]
    }
   ],
   "source": [
    "d = np.diag((1, 2, 3))\n",
    "print(d)\n",
    "w, v = np.linalg.eig(np.dot(d.tranpose(), d))\n",
    "v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(array(1), array(0), array(3))\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def pca(data):\n",
    "    \"\"\"\n",
    "    Given a matrix of data, returns sorted eigenvalues and eigenvectors of covariance matrix\n",
    "    \n",
    "    Hint: This function should use your get_covariance() function and you might find np.linalg.eig() \n",
    "            and np.argsort() helpful\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    # Your code here\n",
    "    covMat = get_covariance(data)\n",
    "    w, v = np.linalg.eig(np.dot(covMat.transpose(), covMat))\n",
    "    ind = np.argsort(w)\n",
    "    ind = ind[::-1]\n",
    "    w[::-1].sort()\n",
    "    eigen_vals = w\n",
    "    eigen_vectors = v.copy()\n",
    "    n = len(ind)\n",
    "    for i in range(0, n):\n",
    "        eigen_vectors[:,i] = v[:,ind[i]]\n",
    "    \n",
    "    \n",
    "    return eigen_vals, eigen_vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def pca_transform(data, num_dims = 2):\n",
    "    \"\"\"\n",
    "    Given a matrix of data, returns data projected onto num_dims principal components (use your pca() function)\n",
    "\n",
    "    \"\"\"\n",
    "    # Your code here\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    return transformed_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def plot_pca(data, graph_title='Transformed Data'):\n",
    "    \"\"\"\n",
    "    Given a matrix of data, runs PCA and plots transformed data onto first 2 principal components\n",
    "    \n",
    "    \"\"\"\n",
    "    labels = np.argmax(data, axis=1)\n",
    "    reduced_data = pca_transform(data)\n",
    "\n",
    "    scatter = plt.scatter(reduced_data[:,0], reduced_data[:,1], c = labels)\n",
    "    plt.legend(*scatter.legend_elements())\n",
    "    plt.title(graph_title)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using Principal Components Analysis to visualize structure\n",
    "\n",
    "Now that we have our PCA up and running, we can gain some intuition on what our results might be showing us. From our visualization, we want to be able to determine if tweets are grouped tightly around a series of distinct topics (structured) or if tweets are not clearly defined by any one topic (unstructured).\n",
    "\n",
    "When we do LDA topic modeling, we are assuming that tweets are the results of samples from an underlying dirichlet distribution. Therefore we can make data that \"looks\" structured or unstructured by sampling from a corresponding Dirichlet distribution. In the structured data visualization we are sampling from a distribution with a high likelihood of a (1.0,0,0,...,0) distribution, (or some permutation of that result), and in the unstructured data visualization we are sampling from a distribution with a high probability of a uniform vector.\n",
    "\n",
    "__NOTE:__ If you are getting an error with the plotting function, try updating matplotlib using the command ```pip install --upgrade matplotlib``` in your command line. Restart your notebook before trying again. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "structured_data = np.random.dirichlet(np.ones(5)*.05, 800)\n",
    "unstructured_data = np.random.dirichlet(np.ones(5)*100, 800)\n",
    "\n",
    "plot_pca(structured_data, graph_title='Structured Data')\n",
    "plot_pca(unstructured_data, graph_title='Unstructured Data')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see from the resulting graphs that PCA does a pretty good job revealing whether there is a lot of structure or if there is very little structure. Taking this information, we can compare our results from twitter to these graphs to get a better idea of the structure of our results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TASK 3: Visualize Results of LDA with PCA\n",
    "\n",
    "Now using the [documentation](https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.LatentDirichletAllocation.html) for sklearn's LDA and the given ```plot_pca()``` function, visualize the structure of the topics of the tweets. \n",
    "\n",
    "Hint: You need to ```transform``` your vectorized tweets into topic space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Your code here\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tweets Over Time\n",
    "\n",
    "Now that we have an understanding of LDA, how the code works, and a way to evaluate it, we can look into ways to measure change in topics over time. We will look at tweets from February, March and April. These tweets were collected over a 6 day interval about 25 days apart. \n",
    "\n",
    "To observe change over time, you will perform LDA on a data set with all three months of tweets. Then, for each topic, we can look at the frequency of tweets from each month to see if some topics were more popular at different points in time. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Loading tweets\n",
    "feb_tweets = pd.read_csv('feb_data.csv')\n",
    "march_tweets = pd.read_csv('march_data.csv')\n",
    "april_tweets = pd.read_csv('april_data.csv')\n",
    "\n",
    "all_tweets = pd.concat([feb_tweets, march_tweets, april_tweets]).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TASK 4: Perform LDA on  All Tweets\n",
    "\n",
    "The variables that you will need after performing LDA for our visualization are:\n",
    "\n",
    "```all_vectorizer``` - the vectorizer you create in order to get the counts for all the tweets\n",
    "\n",
    "```all_counts``` - the ndarray of vectorized tweets\n",
    "\n",
    "```all_as_topics``` - the trained LDA model. We used 20 topics but you can adjust the number as you see fit. \n",
    "\n",
    "Don't forget to create a new vectorizer and that you can use ```print_topics(as_topic, vectorizer)``` to print the topics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Your code here\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can visualize how topics have changed over time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Classify tweets into topics\n",
    "tweet_dists = all_as_topics.transform(all_counts)\n",
    "tweet_labels = np.argmax(tweet_dists, axis=1)\n",
    "all_tweets = all_tweets.assign(tweet_labels= tweet_labels)\n",
    "\n",
    "# Label tweets by month\n",
    "months = pd.Series([\"February\"] * feb_tweets.shape[0] + ['March'] * march_tweets.shape[0] + ['April'] * april_tweets.shape[0])\n",
    "all_tweets = all_tweets.assign(month=months)\n",
    "\n",
    "# Get monthly counts for each topic\n",
    "topic_counts = all_tweets.groupby(['tweet_labels','month']).count()\n",
    "topic_counts.drop(['date','text'], axis=1, inplace=True)\n",
    "topic_counts.columns =['monthly_count']\n",
    "\n",
    "unstacked_topics = topic_counts.unstack()\n",
    "unstacked_topics = unstacked_topics.monthly_count\n",
    "unstacked_topics = unstacked_topics.reindex(columns=['February', 'March', 'April'])\n",
    "\n",
    "unstacked_topics.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Plot monthly counts for each topic\n",
    "unstacked_topics.plot.bar(figsize=(15,10))\n",
    "plt.title('Monthly Tweet Counts by Topic')\n",
    "plt.xlabel('Topic')\n",
    "plt.ylabel('Tweet Count')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TASK 5: Discuss Results\n",
    "\n",
    "Now it is time to do qualitative analysis of your topic modeling. You final task is to look at the graph above and choose a topic with a majority from each month and a topic that seems to be present in all months (4 total topics) and assign a semantic meaning to each based on the words associated with that topic (when you print the topics using print_topics() they are printed in the same order as the graph). For example if the words were ['test', 'positive', 'negative'...] you could say 'Testing for COVID-19'. Comment on whether the results on the bar graph make sense after assigning the semantic meaning to the topic. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
